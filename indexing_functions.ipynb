{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necesssary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from geopy.geocoders import Nominatim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining an es object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "es = Elasticsearch(['http://localhost:9200/'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the mapping and the setting for the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"autocomplete\",\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"text\"},\n",
    "                },\n",
    "            },\n",
    "            \"date\": {\n",
    "                \"type\": \"date\",\n",
    "            },\n",
    "            \"geopoint\": {\n",
    "                \"type\": \"geo_point\",\n",
    "            },\n",
    "            \"temporalExpressions\": {\n",
    "                \"type\": \"text\",\n",
    "            },\n",
    "            \"georeferences\": {\n",
    "                \"type\": \"text\",\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"autocomplete\": {\n",
    "                    \"tokenizer\": \"autocomplete\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                },\n",
    "                \"autocomplete_search\": {\n",
    "                    \"tokenizer\": \"lowercase\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"autocomplete\": {\n",
    "                    \"type\": \"edge_ngram\",\n",
    "                    \"min_gram\": 1,\n",
    "                    \"max_gram\": 25,\n",
    "                    \"token_chars\": [\"letter\", \"digit\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "index_name = \"new_index\"\n",
    "es.indices.create(index=index_name, body=index_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the of publishing the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def extract_date_of_publish(reuters_tag):\n",
    "    date_obj = None\n",
    "    \n",
    "    date_tags = reuters_tag.find_all('date')\n",
    "    \n",
    "    if date_tags:\n",
    "        date_tag = date_tags[0]\n",
    "        date_str = date_tag.text.strip()\n",
    "\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, \"%d-%b-%Y %H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            # Handle the case where the date string is not in the expected format\n",
    "            print(f\"Error: Unable to parse date string '{date_str}'\")\n",
    "    \n",
    "    return date_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_title(reuters_tag):\n",
    "    title_tag = reuters_tag.find('title')\n",
    "    title = title_tag.text.strip() if title_tag else None\n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the temporal expressions in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_expressions(tag):\n",
    "    body_tag = tag.find('body')\n",
    "    \n",
    "    if body_tag:\n",
    "        content = body_tag.text.strip()\n",
    "        doc = nlp(content)\n",
    "        temporal_expressions = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "        return temporal_expressions\n",
    "    else:\n",
    "        return None \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the authors of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_authors(article_tag):\n",
    "    authors_tags = article_tag.find_all('author')\n",
    "    \n",
    "    authors_list = []\n",
    "    for author_tag in authors_tags:\n",
    "        author_name = author_tag.text.strip()\n",
    "        authors_list.append({\"name\": author_name})\n",
    "    \n",
    "    return authors_list if authors_list else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the georeferences in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_georeferences(tag):\n",
    "    content_tag = tag.find('body')\n",
    "    content = content_tag.text.strip() if content_tag else None\n",
    "\n",
    "    if content:\n",
    "        doc = nlp(content)\n",
    "        georeferences = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "        return georeferences\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract and pre proccesing the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_body(article_tag):\n",
    "    body_tag = article_tag.find('body')\n",
    "    \n",
    "    body_text = BeautifulSoup(str(body_tag), 'html.parser').get_text()\n",
    "\n",
    "    # tokenize the content\n",
    "    words = re.findall(r'\\b\\w+\\b', body_text.lower())\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words and len(word) >= 3]\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_words = [porter.stem(word) for word in filtered_words]\n",
    "\n",
    "    # join the processed words back into a string\n",
    "    processed_body = ' '.join(stemmed_words)\n",
    "\n",
    "    return processed_body\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to convert the geo references into geopoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_geopoints(georeferences):\n",
    "    geolocator = Nominatim(user_agent=\"your_user_agent_here\")\n",
    "    coordinates = []\n",
    "\n",
    "    for georef in georeferences:\n",
    "        if georef and isinstance(georef, str):\n",
    "            location = geolocator.geocode(georef)\n",
    "            if location and hasattr(location, 'latitude') and hasattr(location, 'longitude'):\n",
    "                coordinates.append({\n",
    "                    \"lat\": location.latitude,\n",
    "                    \"lon\": location.longitude\n",
    "                })\n",
    "\n",
    "    return coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract and index all attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_document(title, content, authors, date, geopoint, temporal_expressions, georeferences):\n",
    "    try:\n",
    "        # Ensure that required fields have valid values before indexing\n",
    "        if title and content:\n",
    "            document = {\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'authors': authors,\n",
    "                'date': date,\n",
    "                'geopoint': geopoint,\n",
    "                'temporalExpressions': temporal_expressions,\n",
    "                'georeferences': georeferences\n",
    "            }\n",
    "\n",
    "            # Index the document\n",
    "            es.index(index='new_index', body=document)\n",
    "            print(f\"Document indexed successfully: {title}\")\n",
    "        else:\n",
    "            print(\"Skipping document due to missing required fields.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error indexing document: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# index_document(\"Sample Title\", \"Sample Content\", [\"Author1\", \"Author2\"], \"2023-01-01\", \"40.7128,-74.0060\", \"Some Temporal Expressions\", \"Some Georeferences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sgm_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".sgm\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            process_sgm_file(file_path)\n",
    "\n",
    "def process_sgm_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    reuters_tags = soup.find_all('reuters')\n",
    "    for reuters_tag in reuters_tags:      \n",
    "        date=extract_date_of_publish(reuters_tag)\n",
    "        authors=extract_authors(reuters_tag)\n",
    "        title = extract_article_title(reuters_tag)\n",
    "        body = preprocess_body(reuters_tag)\n",
    "        georeferences = extract_georeferences(reuters_tag)\n",
    "        #print(georeferences)\n",
    "        temporal_expressions = extract_temporal_expressions(reuters_tag)\n",
    "        if georeferences is not None:\n",
    "            geopoints = extract_geopoints(georeferences)\n",
    "        \n",
    "        index_document(title, body, authors, date, geopoints, temporal_expressions, georeferences)        \n",
    "\n",
    "data = \"data\"\n",
    "process_sgm_folder(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
